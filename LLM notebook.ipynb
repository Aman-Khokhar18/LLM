{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=10000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.merges = {}\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        # Initialize with characters\n",
    "        all_chars = set()\n",
    "        for text in texts:\n",
    "            all_chars.update(text)\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        vocab = {c: i + len(self.special_tokens) for i, c in enumerate(all_chars)}\n",
    "        vocab.update(self.special_tokens)\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {i: c for c, i in vocab.items()}\n",
    "        \n",
    "        # Tokenize corpus into character pairs\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            word_counts.update(text.split())\n",
    "        \n",
    "        # Implement BPE algorithm\n",
    "        for _ in range(self.vocab_size - len(vocab)):\n",
    "            pairs = self._get_stats(word_counts)\n",
    "            if not pairs:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self._merge_vocab(best_pair, word_counts)\n",
    "            \n",
    "            # Add the merge to vocabulary\n",
    "            self.merges[best_pair] = len(self.vocab)\n",
    "            self.vocab[\"\".join(best_pair)] = len(self.vocab)\n",
    "            self.reverse_vocab[len(self.vocab) - 1] = \"\".join(best_pair)\n",
    "            \n",
    "    def _get_stats(self, word_counts):\n",
    "        pairs = Counter()\n",
    "        for word, count in word_counts.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += count\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab(self, pair, word_counts):\n",
    "        bigram = \" \".join(pair)\n",
    "        replacement = \"\".join(pair)\n",
    "        \n",
    "        new_word_counts = Counter()\n",
    "        for word, count in word_counts.items():\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_word_counts[new_word] = count\n",
    "            \n",
    "        word_counts.clear()\n",
    "        word_counts.update(new_word_counts)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        for char in text:\n",
    "            tokens.append(self.vocab.get(char, self.special_tokens[\"<UNK>\"]))\n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.reverse_vocab.get(token, \"<UNK>\") for token in tokens])\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, seq_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        if not self.tokenizer.vocab:\n",
    "            self.tokenizer.fit([text])\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        self.data = self.tokenizer.encode(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.seq_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_length]\n",
    "        y = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register buffer (not a parameter, but should be saved)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        # x shape: [batch_size, seq_length, embedding_dim]\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        # Q, K, V shapes: [batch_size, num_heads, seq_length, d_k]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask for causal attention (if provided)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        # Reshape to [batch_size, seq_length, num_heads, d_k]\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.d_k)\n",
    "        \n",
    "        # Transpose to [batch_size, num_heads, seq_length, d_k]\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        # Transpose back to [batch_size, seq_length, num_heads, d_k]\n",
    "        batch_size, _, seq_length, _ = x.size()\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Reshape to [batch_size, seq_length, d_model]\n",
    "        return x.contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Linear projections and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply final linear layer\n",
    "        output = self.W_o(self.combine_heads(attention_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Normalize\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def generate_causal_mask(self, seq_length):\n",
    "        # Create causal mask to prevent attending to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool()\n",
    "        return ~mask\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Generate causal mask\n",
    "        causal_mask = self.generate_causal_mask(seq_length).to(x.device)\n",
    "        \n",
    "        # Apply token embedding and positional encoding\n",
    "        x = self.token_embedding(x) * math.sqrt(self.token_embedding.embedding_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask)\n",
    "        \n",
    "        # Apply final output projection\n",
    "        return self.output_projection(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_length=512, dropout=0.1):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "        self.transformer = TransformerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            d_ff=d_ff,\n",
    "            max_seq_length=max_seq_length,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "    \n",
    "    def generate(self, prompt, max_length, tokenizer, temperature=1.0):\n",
    "        self.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_length):\n",
    "            # Get model output\n",
    "            with torch.no_grad():\n",
    "                output = self(input_ids)\n",
    "                \n",
    "            # Get the next token probabilities (last token prediction)\n",
    "            next_token_logits = output[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            # Append to input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Stop if we predict the end of sequence token\n",
    "            if next_token.item() == tokenizer.special_tokens[\"<EOS>\"]:\n",
    "                break\n",
    "                \n",
    "        # Decode tokens to text\n",
    "        generated_text = tokenizer.decode(input_ids[0].tolist())\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_scheduler(optimizer, d_model, warmup_steps=4000):\n",
    "    def lr_lambda(step):\n",
    "        # Linear warmup followed by rsqrt decay\n",
    "        step = max(1, step)  # Prevent division by zero\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (warmup_steps ** -1.5)\n",
    "        return (d_model ** -0.5) * min(arg1, arg2)\n",
    "    \n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def train(model, train_loader, optimizer, scheduler, epochs, clip_value=1.0):\n",
    "    model.train()\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target = target.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1}/{epochs}, Batch: {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f'Epoch: {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "D_FF = 2048\n",
    "MAX_SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 1e-2  \n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BPETokenizer(vocab_size=VOCAB_SIZE)\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset('data.txt', tokenizer, seq_length=MAX_SEQ_LENGTH)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Get actual vocab size from tokenizer\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "# Initialize model\n",
    "model = LanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    max_seq_length=MAX_SEQ_LENGTH\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = get_lr_scheduler(optimizer, D_MODEL)\n",
    "\n",
    "# Train the model\n",
    "losses = train(model, train_loader, optimizer, scheduler, EPOCHS)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.savefig('learning_curve.png')\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'transformer_lm.pth')\n",
    "\n",
    "# Generate text\n",
    "generated_text = model.generate(\"Hello, \", max_length=100, tokenizer=tokenizer, temperature=0.7)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
